@@请问当用户反馈网站访问慢，如何处理？
  1、有哪些方面的因素会导致网站网站访问慢？
    服务器出口带宽不够用
      本身服务器购买的出口带宽比较小。一旦并发量大的话，就会造成分给每个用户的出口带宽就小，访问速度自然就会慢。
      跨运营商网络导致带宽缩减。例如，公司网站放在电信的网络上，那么客户这边对接是长城宽带或联通，这也可能导致带宽的缩减。

    服务器负载过大，导致响应不过来
      以从两个方面入手分析：
        分析系统负载，使用 w 命令或者 uptime 命令查看系统负载。如果负载很高，则使用 top 命令查看 CPU ，MEM 等占用情况，要么是 CPU 繁忙，要么是内存不够。
        如果这二者都正常，再去使用 sar 命令分析网卡流量，分析是不是遭到了攻击。一旦分析出问题的原因，采取对应的措施解决，如决定要不要杀死一些进程，或者禁止一些访问等。

    数据库瓶颈
      如果慢查询比较多。那么就要开发人员或 DBA 协助进行 SQL 语句的优化。
      如果数据库响应慢，考虑可以加一个数据库缓存，如 Redis 等。或者进行读写分离，可以搭建 MySQL 一主多从，一台 MySQL 服务器负责写，其他几台从数据库负责读。

    网站开发代码没有优化好
      例如 SQL 语句没有优化，导致数据库读写相当耗时。
      
 
@针对网站访问慢，怎么去排查？
  a、首先要确定是用户端还是服务端的问题。当接到用户反馈访问慢，那边自己立即访问网站看看，
    1、如果自己这边访问快，基本断定是用户端问题，就需要耐心跟客户解释，协助客户解决问题。不要上来就看服务端的问题。一定要从源头开始，逐步逐步往下。
    2、如果访问也慢，那么可以利用浏览器的调试功能，看看加载那一项数据消耗时间过多，是图片加载慢，还是某些数据加载慢。
  b、针对服务器负载情况。查看服务器硬件(网络、CPU、内存)的消耗情况。如果是购买的云主机，比如阿里云，可以登录阿里云平台提供各方面的监控，比如 CPU、内存、带宽的使用情况。
  c、如果发现硬件资源消耗都不高，那么就需要通过查日志，比如看看 MySQL慢查询的日志，看看是不是某条 SQL 语句查询慢，导致网站访问慢。
      
@怎么去解决？
  如果是出口带宽问题，那么久申请加大出口带宽。
  如果慢查询比较多，那么就要开发人员或 DBA 协助进行 SQL 语句的优化。
  如果数据库响应慢，考虑可以加一个数据库缓存，如 Redis 等等。然后也可以搭建MySQL 主从，一台 MySQL 服务器负责写，其他几台从数据库负责读。
  申请购买 CDN 服务，加载用户的访问。
  如果访问还比较慢，那就需要从整体架构上进行优化咯。做到专角色专用，多台服务器提供同一个服务。 
 
 
@Linux 性能调优都有哪几种方法？
  Disabling daemons (关闭 daemons)。
  Shutting down the GUI (关闭 GUI)。
  Changing kernel parameters (改变内核参数)。
  Kernel parameters (内核参数)。
  Tuning the processor subsystem (处理器子系统调优)。
  Tuning the memory subsystem (内存子系统调优)。
  Tuning the file system (文件系统子系统调优)。
  Tuning the network subsystem（网络子系统调优)。 
 
 
@@在浏览器中输入URL到你所看到的网页，讲一下经过了什么操作？
@要搞懂这个问题，我们需要先解决下面五个问题：
  1、现代浏览器在与服务器建立了一个 TCP 连接后是否会在一个 HTTP 请求完成后断开？什么情况下会断开？
  2、一个 TCP 连接可以对应几个 HTTP 请求？
  3、一个 TCP 连接中 HTTP 请求发送可以一起发送么（比如一起发三个请求，再三个响应一起接收）？
  4、为什么有的时候刷新页面不需要重新建立 SSL 连接？
  5、浏览器对同一 Host 建立 TCP 连接到数量有没有限制？
  
  第一个问题：
    在 HTTP/1.0 中，一个服务器在发送完一个 HTTP 响应后，会断开 TCP 链接。但是这样每次请求都会重新建立和断开 TCP 连接，代价过大。但可以让服务器  \
     支持Connection: keep-alive 的header进行了支持。即完成这个 HTTP 请求之后，不要断开 HTTP 请求使用的 TCP 连接。这样的好处是连接可以被重新使用， \ 
     之后发送 HTTP 请求的时候不需要重新建立 TCP 连接。
    在HTTP/1.1 中默认开启了持久连接，除非请求中写明：Connection: close
    
  第二个问题：
    如果维持连接，一个 TCP 连接是可以发送多个 HTTP 请求的。
    
  第三个问题：
    HTTP/1.1 存在一个问题，单个 TCP 连接在同一时刻只能处理一个请求，意思是说：两个请求的生命周期不能重叠，任意两个 HTTP 请求从开始到结束的时间在同一个 TCP 连接里不能重叠。 \
     虽然可以使用pipelining解决，但又有其他问题，如无法确定响应是对应的那个请求，但浏览器默认关闭pipelining
    答案：在 HTTP/1.1 存在 Pipelining 技术可以完成这个多个请求同时发送，但是由于浏览器默认关闭，所以可以认为这是不可行的。   \
     在 HTTP2 中由于 Multiplexing 特点的存在，多个 HTTP 请求可以在同一个 TCP 连接中并行进行。
   
  第四个问题：
    在第一个问题的讨论中已经有答案了，TCP 连接有的时候会被浏览器和服务端维持一段时间。TCP 不需要重新建立，SSL 自然也会用之前的。
    
  第五个问题：
    答案是：有。Chrome 最多允许对同一个 Host 建立六个 TCP 连接。不同的浏览器有一些区别。
    
@现在给你三百台服务器，你怎么对他们进行管理？
  管理300台服务器的方式：
    1）设定跳板机，使用统一账号登录，便于安全与登录的考量。
    2）使用salt、ansiable、puppet进行系统的统一调度与配置的统一管理。
    3）建立简单的服务器的系统、配置、应用的cmdb信息管理。便于查阅每台服务器上的各种信息记录。  

@简述raid0 raid1 raid5 三种工作模式的工作原理及特点。
  RAID，可以把硬盘整合成一个大磁盘，还可以在大磁盘上再分区，放数据还有一个大功能，多块盘放在一起可以有冗余（备份）RAID整合方式有很多，常用的：0 1 5 10
  
  RAID 0，可以是一块盘和N个盘组合 
    优点：读写快，是RAID中最好的；
    缺点：没有冗余，一块坏了数据就全没有了；

  RAID 1，只能2块盘，盘的大小可以不一样，以小的为准
    10G+10G只有10G，另一个做备份。它有100%的冗余，缺点：浪费资源，成本高
  
  RAID 5 ，3块盘，容量计算10*（n-1）,损失一块盘   特点：读写性能一般，读还好一点，写不好
    
  冗余从好到坏：RAID1 RAID10 RAID5 RAID0
  性能从好到坏：RAID0 RAID10 RAID5 RAID1
  成本从低到高：RAID0 RAID5 RAID1 RAID10
  
@LVS、Nginx、HAproxy有什么区别？工作中你怎么选择？
  LVS： 是基于四层的转发
  HAproxy： 是基于四层和七层的转发，是专业的代理服务器
  Nginx： 是WEB服务器，缓存服务器，又是反向代理服务器，可以做七层的转发，
          【新版本的nginx也可以做四层转发：--with-stream  stream{server{} upstream{}}  这个stream与http同级】

  区别： LVS由于是基于四层的转发所以只能做端口的转发而基于URL的、基于目录的这种转发LVS就做不了。

  工作选择：
  HAproxy和Nginx由于可以做七层的转发，所以URL和目录的转发都可以做。在很大并发量的时候我们就要选择LVS，像中小型公司的话并发量没那么大选择HAproxy或者  \
   Nginx足已，由于HAproxy由是专业的代理服务器配置简单，所以中小型企业推荐使用HAproxy。

@什么是CDN？
  即内容分发网络
  - 其目的是通过在现有的Internet中增加一层新的网络架构，将网站的内容发布到最接近用户的网络边缘，使用户可就近取得所需的内容，提高用户访问网站的速度。

@什么叫网站灰度发布？
  灰度发布是指在黑与白之间，能够平滑过渡的一种发布方式AB test就是一种灰度发布方式，让一部用户继续用A，一部分用户开始用B如果用户对B没有什么反对意见，  \
   那么逐步扩大范围，把所有用户都迁移到B上面 来灰度发布可以保证整体系统的稳定，在初始灰度的时候就可以发现、调整问题，以保证其影响度。



















