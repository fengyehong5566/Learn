# ceph 分布式存储集群

Ceph支持对象存储、块存储、文件系统。
Ceph会把待存储的数据流切分为一个或多个固定大小（默认4M）的数据对象，并以原子单元完成数据的读写

对象数据的底层存储服务是有多个主机组成的存储集群，该集群为RADOS
【https://blog.csdn.net/m0_64417032/article/details/123596825】

## ceph组件：
	cephmon：维护集群状态。维护存储池和pg的映射关系、客户端到管理守护进程之间的身份认证、通常需要多个节点部署实现高可用架构
		
	cephmgr：监控集群运行指标和集群状态，如存储使用率，系统负载，cephmgr内有多种组件如ceph-dashboard仪表盘组件
	
	cephosd：为ceph集群提供存储数据，一个磁盘就是一个osd守护进程，实现数据存储，平衡，恢复等功能。
	
	cephmds：提供文件系统，对象存储和块存储不需要cephmds组件
	
	cephdeploy：通过cephdeploy执行ceph各种命令行工具：rados、ceph、rbd，该节点有admin账户只对管理员开放

## ceph 数据存储架构
  ![image](https://user-images.githubusercontent.com/30826449/186055070-d70269cc-7d63-4bde-abeb-4fb7ca6a58ef.png)

  ![image](https://user-images.githubusercontent.com/30826449/186054932-a4380035-aba7-4501-bbd5-2b2f4f51dd0a.png)

      在ceph中，一切皆对象。无论是那种接口【RDB（块存储接口）、RGW（对象存储接口）、CephFS（文件存储接口）】，其存储到集群中的数据均可看做是一个对象，一个
  文件需要切割为多个对象，然后存储到osd中。
  
  一个文件的存储过程：
      文件  ---（切割）---> n个4M大小的对象  ---（hash算法）---> pg pool  --- （crush算法）---> osd
    1）文件切割成4M大小的数据对象
    2）数据对象通过hash算法找到存储池中的pg
    3）在通过crush算法找到pg映射的osd
    4）pg中的主osd将对象写入到磁盘
    5）主osd将数据同步给备份osd,并等到备份osd写完后返回确认消息
    6）主osd将写完信息告知客户端

# ceph 集群搭建
	>1、【各个节点】创建ceph用户
  >2、【在mgr服务器上】用ceph用户生成密钥对  
  >3、【在mgr服务器上】将公钥同步到osd节点  
  >4、【各个节点】修改/etc/hosts文件，添加主机名和ip的解析  
  >5、【各个节点】配置ceph的yum源  
  >6、【各个节点】安装ceph依赖项   
  >    yum -y install  python2-subprocess32   python36-werkzeug  
  >    pip3 install pecan -i https://pypi.douban.com/simple   
  >7、【各个节点】添加ceph用户免密执行root权限  
  >8、【在mgr服务器上】创建ceph集群配置文件  
  >    yum -y install ceph-deploy  
  >    su - ceph  
  >    ceph-deploy new --cluster-network IPs   --public-network IPs  cephmgr   #会在ceph家目录下生成一个ceph.conf文件  
  >      --cluster-network 为集群内部地址  
  >      --public-network 为外部可以调用的地址  
  >9、【在mgr服务器上】修改ceph.conf文件  
  >10、【各个mon节点】安装ceph-mon组件  
  >    yum -y install ceph-mon  
  >11、【在mgr服务器上】在ceph用户下初始化ceph-mon主节点  
  >    ceph-deploy mon create-initial  
  >    ps aux | grep mon   #在mon服务器上执行  
  >    
  >12、【在mgr服务器上】安装ceph-mgr组件  
  >    yum -y install ceph-mgr  
  >13、【在mgr服务器上】在 ceph用户下初始化mgr节点  
  >    ceph-deploy mgr create cephmgr  
  >    ps  aux | grep  mgr  
  >    
  >14、【在mgr服务器上】在ceph用户下，初始化每个osd节点  
  >    ceph-deploy install  --no-adjust-repos  --nogpgcheck  osdHostName1  
  >    ceph-deploy install  --no-adjust-repos  --nogpgcheck  osdHostName2  
  >    ceph-deploy install  --no-adjust-repos  --nogpgcheck  osdHostName3  
  >15、【在mgr服务器上】在ceph用户下，执行擦除磁盘数据  
  >    ceph-deploy  disk  zap  osdHostName1  /dev/sdb  
  >    ceph-deploy  disk  zap  osdHostName1  /dev/sdc  
  >    ......  
  >16、【在mgr服务器上】在ceph用户下，添加磁盘为osd  
  >    ceph-deploy osd create osdHostName1 --data /dev/sdb  
  >    ceph-deploy osd create osdHostName2 --data /dev/sdc  
  >    ......
  >17、【在mgr服务器上】将ceph家目录下的ceph.conf 和 ceph.client.admin.keyring 文件移动到/etc/ceph目录下，切换到root用户  
  >    ceph -s   #查看集群信息  

## 处理  “mon  is allowing insecure global_id reclaim”警告
  翻译：mon允许不安全的global_id回收
  解决：执行命令
    ceph config  set  mon auth_allow_insecure_global_id_reclaim false
    

## 扩展ceph集群 —— ceph-mon
```shell
yum -y install ceph-mon   //在新的mon节点上执行
ceph-deploy  mon  add  monHostName2   //在mgr服务器ceph用户下执行
ceph -s   //在mgr服务器root用户下执行   --在services下,mon 行会有新的信息
```

## 扩展ceph集群 —— ceph-mgr
```shell
yum -y install ceph-mon   //在新的mgr节点上执行
ceph-deploy mgr create  mgrHostName2  //在mgr服务器ceph用户下执行
ceph -s   //在mgr服务器root用户下执行   --在services下,mgr 行会有新的信息

//验证
systemctl  stop ceph-mgr.target   //在主mgr服务器root用户下执行

```

## ceph集群管理
	pool：存储池的大小取决于底层的存储空间
	pg：抽象的逻辑概念，一个pool中可以有多少个pg，可以通过公式计算
	pgp：是没分数据备份组合

	ceph 集群部署好之后 , 要先创建存储池才能向 ceph 写入数据，文件在向 ceph 保存之前要先进行文件大小的切割，通常切割为大小为4M的存储快，然后在进行一致性 hash 计算，计算后会把文件保存在某个对应的 pg 的osd上，此文件一定属于某个 pool 的一个 pg 。


```shell
ceph osd pool create mypool 32  32
#mypool 为存储池的名称
#第一个 32 为pb数量
#第二个 32 为pgp数量

# 查看存储池的创建情况
ceph pg ls-by-pool mypool | awk '{print $1, $2, $15}'
#输出信息：
#	PG：pg的ID.NUM
#	OBJECTS：
# ACTING：一份数据为一主两备，存储的位置信息

#查看所有poll名称信息
ceph  osd  pool  ls

#查看某个具体pool的详细信息
ceph osd  pool  stats  mypool

#查看存储池副本数配置
ceph osd  pool  get  mypool  size

#设置存储池数据存储副本数
ceph osd pool set mypool size 2
#执行ceph pg ls-by-pool mypool | awk '{print $1, $2, $15}'  命令，可以看到acting列变成了2


```
## 删除存储池
默认情况下ceph集群是不允许删除存储池的，要下删除需要修改系统配置项，允许执行删除存储池操作
```shell
#告诉所有节点可以删除存储池
ceph  tell mon.*  injectargs  --mon-allow-pool-delete=true

#执行删除操作
ceph  osd  pool  delete mypool   mypool  --yes-i-really-really-men-it

#删除后关闭允许删除存储池配置项
ceph  tell mon.*  injectargs  --mon-allow-pool-delete=false
```

## OSD 管理  
```shell
#列出osd信息
ceph  osd stat

#列出osd更详细信息
ceph osd  dump

#列出osd的磁盘列表信息
ceph osd  tree

#停用某个osd
ceph  osd  out  osd.0      #ceph osd tree 输出信息里，osd.0 对应的REWEIGHT的值为0，说明不会在有数据写入
systemctl  stop  ceph-osd@0.service   #【登录osd.0 所在的服务器】，停止进行
ceph osd purge  [id]  --yes-i-reall-mean-it  # [id]为osd的id
ceph -s  #验证


```
## 重新挂载osd.0【也可以作为新增的操作】
执行命令： dmsetup  remove_all   移除下面的信息

![image](https://user-images.githubusercontent.com/30826449/186122765-f255daae-8dff-4b57-b8c9-f20497fbe3f0.png)

```shell
ceph-deploy osd  create  osdHostname4  --data  /dev/sdb
```
>注意：此时会报错：Failed to execute command: /usr/sbin/ceph-volume --cluster ceph lvm create --bluestore --data
>示添加失败了,原因是磁盘里有数据我们要手动清理掉硬盘的分区表 (当然如果硬盘是全新的, 这里应该就成功了).
>执行  dd if=/dev/zero of=/dev/sdb bs=512K count=1

参考网址：
https://blog.csdn.net/m0_64417032/article/details/123668233?spm=1001.2014.3001.5502









