# ceph 分布式存储集群

Ceph支持对象存储、块存储、文件系统。
Ceph会把待存储的数据流切分为一个或多个固定大小（默认4M）的数据对象，并以原子单元完成数据的读写

对象数据的底层存储服务是有多个主机组成的存储集群，该集群为RADOS
【https://blog.csdn.net/m0_64417032/article/details/123596825】

## ceph组件：
	cephmon：维护集群状态。维护存储池和pg的映射关系、客户端到管理守护进程之间的身份认证、通常需要多个节点部署实现高可用架构
		
	cephmgr：监控集群运行指标和集群状态，如存储使用率，系统负载，cephmgr内有多种组件如ceph-dashboard仪表盘组件
	
	cephosd：为ceph集群提供存储数据，一个磁盘就是一个osd守护进程，实现数据存储，平衡，恢复等功能。
	
	cephmds：提供文件系统，对象存储和块存储不需要cephmds组件
	
	cephdeploy：通过cephdeploy执行ceph各种命令行工具：rados、ceph、rbd，该节点有admin账户只对管理员开放

## ceph 数据存储架构
  ![image](https://user-images.githubusercontent.com/30826449/186055070-d70269cc-7d63-4bde-abeb-4fb7ca6a58ef.png)

  ![image](https://user-images.githubusercontent.com/30826449/186054932-a4380035-aba7-4501-bbd5-2b2f4f51dd0a.png)

      在ceph中，一切皆对象。无论是那种接口【RDB（块存储接口）、RGW（对象存储接口）、CephFS（文件存储接口）】，其存储到集群中的数据均可看做是一个对象，一个
  文件需要切割为多个对象，然后存储到osd中。
  
  一个文件的存储过程：
      文件  ---（切割）---> n个4M大小的对象  ---（hash算法）---> pg pool  --- （crush算法）---> osd
    1）文件切割成4M大小的数据对象
    2）数据对象通过hash算法找到存储池中的pg
    3）在通过crush算法找到pg映射的osd
    4）pg中的主osd将对象写入到磁盘
    5）主osd将数据同步给备份osd,并等到备份osd写完后返回确认消息
    6）主osd将写完信息告知客户端

# ceph 集群搭建
  >1、【各个节点】创建ceph用户
  >2、【在mgr服务器上】用ceph用户生成密钥对
  >3、【在mgr服务器上】将公钥同步到osd节点
  >4、【各个节点】修改/etc/hosts文件，添加主机名和ip的解析
  >5、【各个节点】配置ceph的yum源
  >6、【各个节点】安装ceph依赖项
  >    yum -y install  python2-subprocess32   python36-werkzeug
  >    pip3 install pecan -i https://pypi.douban.com/simple 
  >7、【各个节点】添加ceph用户免密执行root权限
  >8、【在mgr服务器上】创建ceph集群配置文件
  >    yum -y install ceph-deploy
  >    su - ceph
  >    ceph-deploy new --cluster-network IPs   --public-network IPs  cephmgr   //会在ceph家目录下生成一个ceph.conf文件
  >      --cluster-network 为集群内部地址
  >      --public-network 为外部可以调用的地址
  >9、【在mgr服务器上】修改ceph.conf文件
  >10、【各个mon节点】安装ceph-mon组件
  >    yum -y install ceph-mon
  >11、【在mgr服务器上】在ceph用户下初始化ceph-mon主节点
  >    ceph-deploy mon create-initial
  >    ps aux | grep mon   //在mon服务器上执行
  >    
  >12、【在mgr服务器上】安装ceph-mgr组件
  >    yum -y install ceph-mgr
  >13、【在mgr服务器上】在 ceph用户下初始化mgr节点
  >    ceph-deploy mgr create cephmgr
  >    ps  aux | grep  mgr
  >    
  >14、【在mgr服务器上】在ceph用户下，初始化每个osd节点
  >    ceph-deploy install  --no-adjust-repos  --nogpgcheck  osdHostName1
  >    ceph-deploy install  --no-adjust-repos  --nogpgcheck  osdHostName2
  >    ceph-deploy install  --no-adjust-repos  --nogpgcheck  osdHostName3
  >15、【在mgr服务器上】在ceph用户下，执行擦除磁盘数据
  >    ceph-deploy  disk  zap  osdHostName1  /dev/sdb
  >    ceph-deploy  disk  zap  osdHostName1  /dev/sdc
  >    ......
  >16、【在mgr服务器上】在ceph用户下，添加磁盘为osd
  >    ceph-deploy osd create osdHostName1 --data /dev/sdb
  >    ceph-deploy osd create osdHostName2 --data /dev/sdc
  >    ......
  >17、【在mgr服务器上】将ceph家目录下的ceph.conf 和 ceph.client.admin.keyring 文件移动到/etc/ceph目录下，切换到root用户
  >    ceph -s   //查看集群信息

## 处理  “mon  is allowing insecure global_id reclaim”警告
  翻译：mon允许不安全的global_id回收
  解决：执行命令
    ceph config  set  mon auth_allow_insecure_global_id_reclaim false
    

## 扩展ceph集群 —— ceph-mon
```shell
yum -y install ceph-mon   //在新的mon节点上执行
ceph-deploy  mon  add  monHostName2   //在mgr服务器ceph用户下执行
ceph -s   //在mgr服务器root用户下执行   --在services下,mon 行会有新的信息
```

## 扩展ceph集群 —— ceph-mgr
```shell
yum -y install ceph-mon   //在新的mgr节点上执行
ceph-deploy mgr create  mgrHostName2  //在mgr服务器ceph用户下执行
ceph -s   //在mgr服务器root用户下执行   --在services下,mgr 行会有新的信息

//验证
systemctl  stop ceph-mgr.target   //在主mgr服务器root用户下执行

```
















