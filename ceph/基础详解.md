# ceph 分布式存储集群

Ceph支持对象存储、块存储、文件系统。
Ceph会把待存储的数据流切分为一个或多个固定大小（默认4M）的数据对象，并以原子单元完成数据的读写

对象数据的底层存储服务是有多个主机组成的存储集群，该集群为RADOS
【https://blog.csdn.net/m0_64417032/article/details/123596825】

## ceph组件：
	cephmon：维护集群状态。维护存储池和pg的映射关系、客户端到管理守护进程之间的身份认证、通常需要多个节点部署实现高可用架构
		
	cephmgr：监控集群运行指标和集群状态，如存储使用率，系统负载，cephmgr内有多种组件如ceph-dashboard仪表盘组件
	
	cephosd：为ceph集群提供存储数据，一个磁盘就是一个osd守护进程，实现数据存储，平衡，恢复等功能。
	
	cephmds：提供文件系统，对象存储和块存储不需要cephmds组件
	
	cephdeploy：通过cephdeploy执行ceph各种命令行工具：rados、ceph、rbd，该节点有admin账户只对管理员开放

## ceph 数据存储架构
  ![image](https://user-images.githubusercontent.com/30826449/186055070-d70269cc-7d63-4bde-abeb-4fb7ca6a58ef.png)

  ![image](https://user-images.githubusercontent.com/30826449/186054932-a4380035-aba7-4501-bbd5-2b2f4f51dd0a.png)

      在ceph中，一切皆对象。无论是那种接口【RDB（块存储接口）、RGW（对象存储接口）、CephFS（文件存储接口）】，其存储到集群中的数据均可看做是一个对象，一个
  文件需要切割为多个对象，然后存储到osd中。
  
  一个文件的存储过程：
      文件  ---（切割）---> n个4M大小的对象  ---（hash算法）---> pg pool  --- （crush算法）---> osd
    1）文件切割成4M大小的数据对象
    2）数据对象通过hash算法找到存储池中的pg
    3）在通过crush算法找到pg映射的osd
    4）pg中的主osd将对象写入到磁盘
    5）主osd将数据同步给备份osd,并等到备份osd写完后返回确认消息
    6）主osd将写完信息告知客户端

# ceph 集群搭建
	>1、【各个节点】创建ceph用户
  >2、【在mgr服务器上】用ceph用户生成密钥对  
  >3、【在mgr服务器上】将公钥同步到osd节点  
  >4、【各个节点】修改/etc/hosts文件，添加主机名和ip的解析  
  >5、【各个节点】配置ceph的yum源  
  >6、【各个节点】安装ceph依赖项   
  >    yum -y install  python2-subprocess32   python36-werkzeug  
  >    pip3 install pecan -i https://pypi.douban.com/simple   
  >7、【各个节点】添加ceph用户免密执行root权限  
  >8、【在mgr服务器上】创建ceph集群配置文件  
  >    yum -y install ceph-deploy  
  >    su - ceph  
  >    ceph-deploy new --cluster-network IPs   --public-network IPs  cephmgr   #会在ceph家目录下生成一个ceph.conf文件  
  >      --cluster-network 为集群内部地址  
  >      --public-network 为外部可以调用的地址  
  >9、【在mgr服务器上】修改ceph.conf文件  
  >10、【各个mon节点】安装ceph-mon组件  
  >    yum -y install ceph-mon  
  >11、【在mgr服务器上】在ceph用户下初始化ceph-mon主节点  
  >    ceph-deploy mon create-initial  
  >    ps aux | grep mon   #在mon服务器上执行  
  >    
  >12、【在mgr服务器上】安装ceph-mgr组件  
  >    yum -y install ceph-mgr  
  >13、【在mgr服务器上】在 ceph用户下初始化mgr节点  
  >    ceph-deploy mgr create cephmgr  
  >    ps  aux | grep  mgr  
  >    
  >14、【在mgr服务器上】在ceph用户下，初始化每个osd节点  
  >    ceph-deploy install  --no-adjust-repos  --nogpgcheck  osdHostName1  
  >    ceph-deploy install  --no-adjust-repos  --nogpgcheck  osdHostName2  
  >    ceph-deploy install  --no-adjust-repos  --nogpgcheck  osdHostName3  
  >15、【在mgr服务器上】在ceph用户下，执行擦除磁盘数据  
  >    ceph-deploy  disk  zap  osdHostName1  /dev/sdb  
  >    ceph-deploy  disk  zap  osdHostName1  /dev/sdc  
  >    ......  
  >16、【在mgr服务器上】在ceph用户下，添加磁盘为osd  
  >    ceph-deploy osd create osdHostName1 --data /dev/sdb  
  >    ceph-deploy osd create osdHostName2 --data /dev/sdc  
  >    ......
  >17、【在mgr服务器上】将ceph家目录下的ceph.conf 和 ceph.client.admin.keyring 文件移动到/etc/ceph目录下，切换到root用户  
  >    ceph -s   #查看集群信息  

## 处理  “mon  is allowing insecure global_id reclaim”警告
  翻译：mon允许不安全的global_id回收
  解决：执行命令
    ceph config  set  mon auth_allow_insecure_global_id_reclaim false
    

## 扩展ceph集群 —— ceph-mon
```shell
yum -y install ceph-mon   //在新的mon节点上执行
ceph-deploy  mon  add  monHostName2   //在mgr服务器ceph用户下执行
ceph -s   //在mgr服务器root用户下执行   --在services下,mon 行会有新的信息
```

## 扩展ceph集群 —— ceph-mgr
```shell
yum -y install ceph-mon   //在新的mgr节点上执行
ceph-deploy mgr create  mgrHostName2  //在mgr服务器ceph用户下执行
ceph -s   //在mgr服务器root用户下执行   --在services下,mgr 行会有新的信息

//验证
systemctl  stop ceph-mgr.target   //在主mgr服务器root用户下执行

```

## ceph集群管理
	pool：存储池的大小取决于底层的存储空间
	pg：抽象的逻辑概念，一个pool中可以有多少个pg，可以通过公式计算
	pgp：是没分数据备份组合

	ceph 集群部署好之后 , 要先创建存储池才能向 ceph 写入数据，文件在向 ceph 保存之前要先进行文件大小的切割，通常切割为大小为4M的存储快，然后在进行一致性 hash 计算，计算后会把文件保存在某个对应的 pg 的osd上，此文件一定属于某个 pool 的一个 pg 。


```shell
ceph osd pool create mypool 32  32
#mypool 为存储池的名称
#第一个 32 为pb数量
#第二个 32 为pgp数量

# 查看存储池的创建情况
ceph pg ls-by-pool mypool | awk '{print $1, $2, $15}'
#输出信息：
#	PG：pg的ID.NUM
#	OBJECTS：
# ACTING：一份数据为一主两备，存储的位置信息

#查看所有poll名称信息
ceph  osd  pool  ls

#查看某个具体pool的详细信息
ceph osd  pool  stats  mypool

#查看存储池副本数配置
ceph osd  pool  get  mypool  size

#设置存储池数据存储副本数
ceph osd pool set mypool size 2
#执行ceph pg ls-by-pool mypool | awk '{print $1, $2, $15}'  命令，可以看到acting列变成了2


```
## 删除存储池
默认情况下ceph集群是不允许删除存储池的，要下删除需要修改系统配置项，允许执行删除存储池操作
```shell
#告诉所有节点可以删除存储池
ceph  tell mon.*  injectargs  --mon-allow-pool-delete=true

#执行删除操作
ceph  osd  pool  delete mypool   mypool  --yes-i-really-really-men-it

#删除后关闭允许删除存储池配置项
ceph  tell mon.*  injectargs  --mon-allow-pool-delete=false
```

## OSD 管理  
```shell
#列出osd信息
ceph  osd stat

#列出osd更详细信息
ceph osd  dump

#列出osd的磁盘列表信息
ceph osd  tree

#停用某个osd
ceph  osd  out  osd.0      #ceph osd tree 输出信息里，osd.0 对应的REWEIGHT的值为0，说明不会在有数据写入
systemctl  stop  ceph-osd@0.service   #【登录osd.0 所在的服务器】，停止进行
ceph osd purge  [id]  --yes-i-reall-mean-it  # [id]为osd的id
ceph -s  #验证


```
## 重新挂载osd.0【也可以作为新增的操作】
执行命令： dmsetup  remove_all   移除下面的信息

![image](https://user-images.githubusercontent.com/30826449/186122765-f255daae-8dff-4b57-b8c9-f20497fbe3f0.png)

```shell
ceph-deploy osd  create  osdHostname4  --data  /dev/sdb
```
>注意：此时会报错：Failed to execute command: /usr/sbin/ceph-volume --cluster ceph lvm create --bluestore --data
>示添加失败了,原因是磁盘里有数据我们要手动清理掉硬盘的分区表 (当然如果硬盘是全新的, 这里应该就成功了).
>执行  dd if=/dev/zero of=/dev/sdb bs=512K count=1

# ceph集群 —— 块存储

说的通俗一点，硬盘就是块存储
在存储池的基础上，启用块存储
```shell
# 创建存储池myrdb1
ceph osd  pool  create  myrdb1 32  32

#查看已创建存储池的信息
ceph  osd pool  ls

#启用块存储
ceph osd pool application enable myrdb1  rbd   

#对块存储进行初始化
rbd  pool  init -p myrdb1   

#创建一个3G大小的磁盘镜像
rdb  create  myimg1  --size 3G  --pool  myrdb1  --image-format 2 --image-feature  layering
#--image-feature 为启用的磁盘镜像特性

#列出创建的磁盘镜像信息
 rbd  --image  myimg1  --pool myrdb1  info
 
```
## 挂载存储块
```shell
#在新服务器上安装 ceph-common
yum install ceph-common

#执行rbd磁盘挂载
rbd --user  admin  -p mypool  map  myimg1
#-p 块myrdb1;  myimg1为镜像名称  

#查看磁盘是否挂载成功
lsblk

```
>注：磁盘挂载需要有权限的用户才能挂载，本示例中采用admin最高权限用户，需要把mgr节点上的ceph.client.admin.keyring和ceph.conf 配置文件放置到的新节点/etc/ceph/ 下

## rbd 镜像回收机制
```shell
#查看myimg1状态
rbd status  --pool  myrdb1  --image myimg1

#如果正在使用，请做好备份，使用 umount命令卸载

#镜像移动到回收站
rbd trash  move --pool myrdb1 --image myimg1

#查看回收站镜像
rbd trash list --pool myrdb1

#从回收站删除镜像
rbd  showmapped   #查看映射关系
rbd  unmap  myrdb1/myimg1    #取消相应的映射
rbd  trash remove --pool myrdb1  IDxxxx   #清除回收站

```

# ceph —— 对象存储

对象存储：各种云都有自己的对象存储系统
```shell
#部署radosgw 并对rgw做高可用和监控
	#在需要安装radosgw服务器上安装rgw组件
	yum install -y  ceph-radosgw
	
	#添加ceph-rdaosgw到集群中
	ceph-deploy  rgw  create sgwHostname1
	ceph-deploy  rgw  create sgwHostname2
	
	#查看集群状态——rgw新增两个节点
	ceph -s

```

## 在rgw 前端添加nginx反向代理层

![image](https://user-images.githubusercontent.com/30826449/186187088-fa91c7c0-61d7-4ae1-b5b0-71d2723ca07d.png)

反向代理层为： IP:7480

## 创建操作对象存储的专属用户
```shell
radosgw-admin  user create --uid="user1" --display-name="test user1"
#--uid 为用户名
#--display-name 为用户说明
#创建完用户后会自动输出用户的access_key 和 secret_key 保存好这两个信息，后面上传时需要用到

```

## 安装 s3cmd 工具
s3cmd工具可用于在命令行操作ceph
```shell
yum install -y  s3cmd
```

**生产s3cmd命令配置工具：**
1、配置hosts域名解析
2、配置s2cmd配置文件
s2cmd --configure

如下图：
![image](https://user-images.githubusercontent.com/30826449/186189173-aeaaeab6-c375-4ebd-a9d0-7a806d4fea4e.png)

解决ERROR: S3 error: 403 (SignatureDoesNotMatch) 问题：
vi /root/.s3cfg
	把 signature_v2 = False 改为 signature_v2 = True

**创建bucket：**
s3cmd  mb  s3：//llh  #创建bucket
s3cmd ls  #查看创建的buckert

上传文件测试：
s3cmd put  file1  s3://llh/img/

查看对象：
s3cmd  ls s3://llh/img/

通过get方式下载文件
s3cmd get  s3://llh/img/file1 /tmp/

删除文件对象
s3cmd  del  s3://llh/img/file1

删除bucket
s3cmd  rb  s3://llh


# ceph集群 —— 文件系统管理

	cephmds: 是 Ceph 分布式文件系统的元数据服务器守护进程。一个或多个 ceph-mds 实例共同管理文件系统命名空间，协调对共享 OSD 集群的访问

```shell
#安装ceph-mds软件
#选取节点安装ceph-mds
yum -y install ceph-mds

#把mds节点加入cephmds集群
ceph-deploy mds  create  osdHostName1
ceph-deploy mds  create  osdHostName2

#查看mds加入集群状况
ceph -s  

#创建cephfs  metadata 和 data 存储池
#创建元数据分区
ceph  osd  pool  create  cephfs-metadata  16 16

#创建数据
ceph  osd  pool  create  cephfs-data 32  32 

#创建一个叫mycephfs的新文件系统
ceph  fs  new  mycephfs  cephfs-metadata  cephfs-data

# 查看文件系统信息
ceph  fs status mycephfs

# 在业务服务器上挂载ceph-fs文件系统
##在业务服务器上安装ceph服务
yum  install ceph

##执行ceph文件系统挂载
mount -t ceph  ip1:6789,ip2:6789,ip2:6789:/  /mnt -o name=admin,secret=xxxxxxxx
#此处的IP地址为mds节点IP地址，secret为/etc/ceph/ceph.client.admin.keyring里key的证书

# ceph-mds 节点提升为双主一备状态配置
ceph fs set mycephfs  max_mds 2

```

# ceph 集群 —— 用户管理

ceph使用cephx协议对客户端进行身份认证，cephx 用于对ceph保存的数据镜像认证和访问授权，用于ceph对请求进行认证和授权，与mon通信的请求都要经过ceph认证，但也可以在mon节点上关闭cephx认证。关闭认证后任何访问都会被允许，但无法保证访问的安全性。

```shell
# 列出ceph账号信息
ceph  auth list

#列出某个账号类型的key
ceph auth get  client.admin

# 添加用户
ceph  auth  add  client.lulu mon 'allow r' osd 'allow rwx pool=myrdb1'
#mon节点可读，osd节点可以读写执行，在pool为myrdb1上

#修改用户权限
ceph  auth caps client.lulu mon 'allow r' osd 'allow rw pool=myrdb1'

#删除用户
ceph  auth del  client.lulu

#生成用户秘钥文件
ceph-authtool  --create-keyring  ceph.client.lulu.keyring  #此时文件为空
ceph auth get  client.tom  -o ceph.client.lulu.keyring

#对于误删除的用户如何导出用户
ceph auth import -i ceph.client.lulu.keyring

```



参考网址：
https://blog.csdn.net/m0_64417032/article/details/123668233?spm=1001.2014.3001.5502









